exp:
  expname: CCTA
  expdir: ./logs/
  dataconfig: ./config/config.yml
  # Batch processing configuration
  input_data_dir: ./data/GT_volumes/  # Directory containing ground truth volumes (1.npy, 2.npy, etc.)
  output_recon_dir: ./logs/reconstructions/  # Directory to save reconstructions (recon_1.npy, etc.)
  model_numbers: [1, 2, 3, 4, 5]  # List of model numbers to train on (corresponds to {number}.npy files)
  current_model_id: 1  # Current model being processed (will be updated automatically)
network:
  net_type: mlp
  num_layers: 8
  hidden_dim: 256
  skips: [4]
  out_dim: 1
  last_activation: linear
  bound: 0.3
  use_gradient_checkpointing: true  # Enable gradient checkpointing for memory efficiency
encoder:
  encoding: hashgrid
  input_dim: 3
  num_levels: 16
  level_dim: 2
  base_resolution: 16
  log2_hashmap_size: 19
render:
  n_fine: 0
  netchunk: 699060
train:
  epoch: 5000
  lrate: [0.0001, 0.00001]  # List of learning rates to experiment with
  lrate_gamma: 0.1
  lrate_step: 5000
  resume: False
  mixed_precision: true  # Enable mixed precision training
  gradient_accumulation_steps: 1  # Can be increased if needed
  weight_decay: 0.000001  # Add weight decay for regularization (1e-6)
  memory_efficient_eval: true  # Use memory-efficient evaluation
  # SDF-related parameters
  use_sdf: true  # Use SDF representation instead of occupancy
  sdf_alpha: 50.0  # Sharpness parameter for SDF to occupancy conversion
  eikonal_weight: 0.1  # Weight for Eikonal regularization loss
  sdf_loss_weight: 1.0  # Weight for 2D SDF projection loss
  # Loss experiment settings
  loss_experiments: ["combined", "projection_only", "sdf_only"]  # List of loss combinations to try
log:
  i_eval: 5000  # Only evaluate at final epoch
  i_save: 5000  # Only save checkpoint at final epoch
  save_final_model: true  # Save final 3D reconstruction and network
  save_intermediate: false  # Don't save intermediate results
